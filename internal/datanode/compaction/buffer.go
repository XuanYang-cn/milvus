package compaction

import (
	"path"
	"strconv"

	"github.com/milvus-io/milvus/internal/datanode/allocator"
	"github.com/milvus-io/milvus/internal/proto/datapb"
	"github.com/milvus-io/milvus/internal/proto/etcdpb"
	"github.com/milvus-io/milvus/internal/storage"

	"github.com/milvus-io/milvus/pkg/common"
	"github.com/milvus-io/milvus/pkg/log"
	"github.com/milvus-io/milvus/pkg/util/metautil"

	"github.com/milvus-io/milvus-proto/go-api/schemapb"
	"go.uber.org/zap"
)

type MutableBuffer interface {
	Write(row Row) error
	Full() bool
	Empty() bool
	Freeze(segID, partitionID UniqueID, meta *etcdpb.CollectionMeta, alloc allocator.Allocator) (map[string][]byte, map[UniqueID]*datapb.FieldBinlog, error)
}

type (
	InsertBuffer struct {
		*storage.InsertData
		maxSize  int64
		rootPath string
	}

	DeltaBuffer struct {
		*storage.DeleteData
		rootPath string
	}

	StatsBuffer storage.PkStatistics
)

var _ MutableBuffer = (*InsertBuffer)(nil)
var _ MutableBuffer = (*DeltaBuffer)(nil)

func NewInsertBuffer(maxSize int64, schema *schemapb.CollectionSchema, chunkManagerRoot string) *InsertBuffer {
	// TODO handling error
	iData, _ := storage.NewInsertData(schema)

	return &InsertBuffer{iData, maxSize, path.Join(chunkManagerRoot, common.SegmentInsertLogPath)}
}

func (i *InsertBuffer) Write(row Row) error {
	// TODO assertion error
	return i.Append(row.(*InsertRow).Value)
}

func (i *InsertBuffer) Empty() bool {
	return i.InsertData.GetRowNum() == 0
}

func (i InsertBuffer) Full() bool {
	return int64(i.GetMemorySize()) >= i.maxSize
}

func (i *InsertBuffer) Freeze(segID, partID UniqueID, meta *etcdpb.CollectionMeta, allocator allocator.Allocator) (map[string][]byte, map[UniqueID]*datapb.FieldBinlog, error) {
	inCodec := storage.NewInsertCodecWithSchema(meta)
	inlogs, _, err := inCodec.Serialize(partID, segID, i.InsertData)
	if err != nil {
		return nil, nil, err
	}

	var (
		kvs     = make(map[string][]byte, len(inlogs))
		inpaths = make(map[UniqueID]*datapb.FieldBinlog)
	)

	notifyGenIdx := make(chan struct{})
	defer close(notifyGenIdx)

	generator, err := allocator.GetGenerator(len(inlogs), notifyGenIdx)
	if err != nil {
		return nil, nil, err
	}

	for _, blob := range inlogs {
		// Blob Key is generated by Serialize from int64 fieldID in collection schema, which won't raise error in ParseInt
		fID, _ := strconv.ParseInt(blob.GetKey(), 10, 64)
		k := metautil.JoinIDPath(meta.GetID(), partID, segID, fID, <-generator)
		key := path.Join(i.rootPath, common.SegmentInsertLogPath, k)

		value := blob.GetValue()
		fileLen := len(value)

		kvs[key] = value
		inpaths[fID] = &datapb.FieldBinlog{
			FieldID: fID,
			Binlogs: []*datapb.Binlog{{LogSize: int64(fileLen), LogPath: key, EntriesNum: blob.RowNum}},
		}
	}

	return kvs, inpaths, nil
}

func NewDeltaBuffer(chunkManagerRoot string) *DeltaBuffer {
	return &DeltaBuffer{
		&storage.DeleteData{
			Pks: make([]storage.PrimaryKey, 0),
			Tss: make([]storage.Timestamp, 0),
		},
		path.Join(chunkManagerRoot, common.SegmentDeltaLogPath),
	}
}

func (d *DeltaBuffer) Write(row Row) error {
	// TODO
	// return d.Append(row)
	return nil
}

// never full, one large file
func (d *DeltaBuffer) Full() bool {
	return false
}

func (d *DeltaBuffer) Empty() bool {
	return d.DeleteData.RowCount == int64(0)
}

func (d *DeltaBuffer) Freeze(segID, partID UniqueID, meta *etcdpb.CollectionMeta, allocator allocator.Allocator) (map[string][]byte, map[UniqueID]*datapb.FieldBinlog, error) {

	var (
		// deltaInfo = make([]*datapb.FieldBinlog, 0)
		kvs    = make(map[string][]byte)
		paths  = make(map[UniqueID]*datapb.FieldBinlog)
		collID = meta.GetID()
	)

	dCodec := storage.NewDeleteCodec()
	blob, err := dCodec.Serialize(collID, partID, segID, d.DeleteData)
	if err != nil {
		return nil, nil, err
	}

	idx, err := allocator.AllocOne()
	if err != nil {
		return nil, nil, err
	}

	key := path.Join(d.rootPath, common.SegmentDeltaLogPath, metautil.JoinIDPath(collID, partID, segID, idx))
	kvs[key] = blob.GetValue()

	binlog := &datapb.FieldBinlog{
		FieldID: 0, // TODO: Not useful on deltalogs, FieldID shall be ID of primary key field
		Binlogs: []*datapb.Binlog{{
			EntriesNum: d.RowCount,
			LogPath:    key,
			LogSize:    int64(len(blob.GetValue())),
		}},
	}
	log.Debug("binlog", zap.Any("binlog", binlog))
	paths[0] = binlog
	return kvs, paths, nil
}

// TODO
func (s *StatsBuffer) Write(row Row) error {
	// return s.Append(row)
	return nil
}

// TODO never full, one large file
func (s *StatsBuffer) Full() bool {
	return false
}

// TODO
func (s *StatsBuffer) Empty() bool {
	return false
}

// TODO
func (s *StatsBuffer) Freeze(segID, partitionID UniqueID, meta *etcdpb.CollectionMeta, alloc allocator.Allocator) (map[string][]byte, map[UniqueID]*datapb.FieldBinlog, error) {
	return nil, nil, nil
}
